{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1N-Yf1N3Z78ZWxbnODL8ImRXU8mrYrJs2",
      "authorship_tag": "ABX9TyN+XQvFuwvTTZDrZjznsCVA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guy-meld-ai/meld-ml/blob/main/MELD_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "XiNNP3mapxoX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df728da0-1d79-4c58-db88-f0efa4653608"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.4.10-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.13.0.90)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.24.0+cpu)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.31.0)\n",
            "Collecting ultralytics-thop>=2.0.18 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.18-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2026.1.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.3)\n",
            "Downloading ultralytics-8.4.10-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.18-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "Successfully installed ultralytics-8.4.10 ultralytics-thop-2.0.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/guy-meld-ai/meld-ml.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HpftQVP4FG1",
        "outputId": "b9d69226-4da2-414e-d394-8c5d5549f576"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'meld-ml'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (3/3), 4.58 MiB | 17.78 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "G9uD8Hanovtp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd6ae9a-6075-4b5b-ca50-491610d18999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI client initialized.\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import base64\n",
        "from ultralytics import YOLO\n",
        "import sys\n",
        "from google.colab import userdata\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "    os.environ['OPENAI_API_KEY'] = userdata.get('OPEN_AI')\n",
        "    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "    print(\"OpenAI client initialized.\")\n",
        "except ImportError:\n",
        "    print(\"Error: 'openai' library not found. Please run 'pip install openai'\")\n",
        "    sys.exit()\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing OpenAI client: {e}\")\n",
        "    print(\"Please make sure your OPENAI_API_KEY environment variable is set correctly.\")\n",
        "    sys.exit()\n",
        "\n",
        "class FrameQualityFilter:\n",
        "    def __init__(self, min_mean=20, max_mean=235, blur_threshold=60, std_threshold=5,\n",
        "                 motion_threshold=0.005, failure_ratio_limit=0.5):\n",
        "        self.min_mean = min_mean\n",
        "        self.max_mean = max_mean\n",
        "        self.blur_threshold = blur_threshold\n",
        "        self.std_threshold = std_threshold\n",
        "        self.motion_threshold = motion_threshold\n",
        "        self.failure_ratio_limit = failure_ratio_limit\n",
        "        self.prev_gray = None\n",
        "\n",
        "    def check_quality(self, frame):\n",
        "        if frame is None: return False, \"Empty\"\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        mean_val, std_val = np.mean(gray), np.std(gray)\n",
        "\n",
        "        if mean_val < self.min_mean or mean_val > self.max_mean:\n",
        "            return False, f\"Bad Exposure ({mean_val:.1f})\"\n",
        "        if std_val < self.std_threshold:\n",
        "            return False, f\"Low Contrast ({std_val:.1f})\"\n",
        "\n",
        "        blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
        "        if blur_score < self.blur_threshold:\n",
        "            return False, f\"Blurry ({blur_score:.1f})\"\n",
        "\n",
        "        is_moving, reason = True, \"Valid\"\n",
        "        if self.prev_gray is not None:\n",
        "            diff = cv2.absdiff(self.prev_gray, gray)\n",
        "            _, thresh = cv2.threshold(diff, 25, 255, cv2.THRESH_BINARY)\n",
        "            changed_ratio = np.count_nonzero(thresh) / float(gray.size)\n",
        "            if changed_ratio < self.motion_threshold:\n",
        "                is_moving, reason = False, f\"Static ({changed_ratio*100:.3f}% change)\"\n",
        "\n",
        "        self.prev_gray = gray\n",
        "        return is_moving, reason\n",
        "\n",
        "class DogBehaviorPipeline:\n",
        "    def __init__(self, model_path, llm_model, quality_filter, question = \"What the dog is doing?\", heartbeat_interval=10, stability_threshold=2):\n",
        "        self.model = YOLO(model_path)\n",
        "        self.llm_model = llm_model\n",
        "        self.filter = quality_filter\n",
        "        self.question = question\n",
        "        self.heartbeat_interval = heartbeat_interval\n",
        "        self.stability_threshold = stability_threshold\n",
        "\n",
        "        self.last_confirmed_label = None\n",
        "        self.current_candidate = None\n",
        "        self.stability_counter = 0\n",
        "        self.frames_since_llm = 0\n",
        "\n",
        "        # State for repetition\n",
        "        self.last_yolo_output = None\n",
        "        self.last_llm_description = None\n",
        "\n",
        "        # Safety: Track consecutive static 'Kept' frames\n",
        "        self.consecutive_kept_count = 0\n",
        "\n",
        "    def get_best_detection(self, results):\n",
        "        for r in results:\n",
        "            if len(r.boxes) == 0: return None\n",
        "            idx = int(r.boxes.conf.argmax())\n",
        "            return {\n",
        "                \"bbox\": r.boxes.xyxy[idx].tolist(),\n",
        "                \"conf\": round(float(r.boxes.conf[idx]), 3),\n",
        "                \"label\": self.model.names[int(r.boxes.cls[idx])]\n",
        "            }\n",
        "        return None\n",
        "\n",
        "    def calculate_iou(self, box1, box2):\n",
        "        # box format: [x1, y1, x2, y2]\n",
        "        x1 = max(box1[0], box2[0])\n",
        "        y1 = max(box1[1], box2[1])\n",
        "        x2 = min(box1[2], box2[2])\n",
        "        y2 = min(box1[3], box2[3])\n",
        "\n",
        "        intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "        union = area1 + area2 - intersection\n",
        "        if union == 0: return 0\n",
        "        return intersection / union\n",
        "\n",
        "    def analyze_frame_with_llm(self, frame_data):\n",
        "        _, buffer = cv2.imencode('.jpg', frame_data)\n",
        "        base64_image = base64.b64encode(buffer).decode('utf-8')\n",
        "        data_url = f\"data:image/jpeg;base64,{base64_image}\"\n",
        "\n",
        "        try:\n",
        "            response = client.responses.create(\n",
        "            model=self.llm_model,\n",
        "            input=[\n",
        "                {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"You are an expert canine behaviorist. Answer with objective information only, make no guesses or assumptions.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": [\n",
        "                            {\"type\": \"input_text\", \"text\": self.question},\n",
        "                            {\"type\": \"input_image\", \"image_url\": data_url, \"detail\": \"high\"}\n",
        "                        ]\n",
        "                    }\n",
        "            ],\n",
        "            text={\n",
        "             \"verbosity\": \"low\"\n",
        "            }\n",
        "\n",
        "        )\n",
        "            return response.output_text\n",
        "        except Exception as e:\n",
        "            return f\"Error analyzing frame: {e}\"\n",
        "\n",
        "    def process_chunk(self, frame_array, chunk_start_dt, fps):\n",
        "        chunk_data = []\n",
        "        dropped = 0\n",
        "        static_preserved_count = 0\n",
        "\n",
        "        for i, frame in enumerate(frame_array):\n",
        "            offset_seconds = i / fps\n",
        "            current_dt = chunk_start_dt + timedelta(seconds=offset_seconds)\n",
        "            timestamp_str = current_dt.strftime(\"%H:%M:%S.%f\")[:-4] # HH:MM:SS.mm\n",
        "\n",
        "            is_valid, reason = self.filter.check_quality(frame)\n",
        "\n",
        "            # --- Force re-check on first frame of chunk ---\n",
        "            if i == 0 and not is_valid and \"Static\" in reason:\n",
        "                is_valid = True\n",
        "                reason = \"Chunk Start Recheck\"\n",
        "            # ----------------------------------------------\n",
        "\n",
        "            # --- Force re-check if static safety limit reached ---\n",
        "            if not is_valid and \"Static\" in reason and self.consecutive_kept_count >= 10:\n",
        "                is_valid = True\n",
        "                reason = \"Static Safety Recheck\"\n",
        "            # ----------------------------------------------\n",
        "\n",
        "            frame_entry = {\n",
        "                \"timestamp\": timestamp_str,\n",
        "                \"status\": \"Valid\",\n",
        "                \"yolo_output\": None,\n",
        "                \"llm_description\": None\n",
        "            }\n",
        "\n",
        "            trigger_llm = False\n",
        "\n",
        "            if is_valid:\n",
        "                results = self.model(frame, verbose=False)\n",
        "                best = self.get_best_detection(results)\n",
        "                current_label = best['label'] if best else \"no_dog\"\n",
        "                frame_entry[\"yolo_output\"] = best\n",
        "\n",
        "                # --- IoU Trigger Check ---\n",
        "                if best and self.last_yolo_output:\n",
        "                    iou = self.calculate_iou(best['bbox'], self.last_yolo_output['bbox'])\n",
        "                    if iou < 0.3:\n",
        "                        trigger_llm = True\n",
        "                        best['label'] = \"walk\"\n",
        "                        current_label = \"walk\"\n",
        "                # -------------------------\n",
        "\n",
        "                # Update state\n",
        "                self.last_yolo_output = best\n",
        "                self.consecutive_kept_count = 0\n",
        "\n",
        "                if current_label != self.last_confirmed_label:\n",
        "                    if current_label == self.current_candidate:\n",
        "                        self.stability_counter += 1\n",
        "                    else:\n",
        "                        self.current_candidate = current_label\n",
        "                        self.stability_counter = 1\n",
        "\n",
        "                    # Check for immediate trigger condition (Initial detection)\n",
        "                    is_initial = (self.last_confirmed_label is None or self.last_confirmed_label == \"no_dog\") and current_label != \"no_dog\"\n",
        "                    threshold = 1 if is_initial else self.stability_threshold\n",
        "\n",
        "                    if self.stability_counter >= threshold:\n",
        "                        self.last_confirmed_label = current_label\n",
        "                        trigger_llm = True\n",
        "\n",
        "            elif \"Static\" in reason and self.last_yolo_output is not None:\n",
        "                # Increment safety counter\n",
        "                self.consecutive_kept_count += 1\n",
        "\n",
        "                # Special handling for Static frames to repeat behavior\n",
        "                frame_entry[\"status\"] = \"Kept\"\n",
        "                frame_entry[\"yolo_output\"] = self.last_yolo_output\n",
        "                static_preserved_count += 1\n",
        "\n",
        "            else:\n",
        "                frame_entry[\"status\"] = \"Dropped\"\n",
        "                frame_entry[\"reason\"] = reason\n",
        "                dropped += 1\n",
        "\n",
        "            # --- Common LLM Logic for Valid and Kept frames ---\n",
        "            if frame_entry[\"status\"] in [\"Valid\", \"Kept\"]:\n",
        "                self.frames_since_llm += 1\n",
        "\n",
        "                # Heartbeat check\n",
        "                if self.frames_since_llm >= self.heartbeat_interval:\n",
        "                    trigger_llm = True\n",
        "\n",
        "                # Only call LLM if trigger is Active AND we have a valid YOLO detection\n",
        "                if trigger_llm and frame_entry[\"yolo_output\"] is not None:\n",
        "                    desc = self.analyze_frame_with_llm(frame)\n",
        "                    frame_entry[\"llm_description\"] = desc\n",
        "                    self.last_llm_description = desc\n",
        "                    self.frames_since_llm = 0\n",
        "            # --------------------------------------------------\n",
        "\n",
        "            chunk_data.append(frame_entry)\n",
        "\n",
        "\n",
        "        return chunk_data, (dropped / len(frame_array))\n",
        "\n",
        "def run_video_to_json(video_path, pipeline, output_dir=\"behavior_analysis_output\", chunk_seconds=30, target_fps=None, video_start_time=\"00:00:00\"):\n",
        "    if not os.path.exists(output_dir): os.makedirs(output_dir)\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    video_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    if video_fps <= 0: video_fps = 30\n",
        "\n",
        "    # Parse start time\n",
        "    try:\n",
        "        start_dt = datetime.strptime(video_start_time, \"%H:%M:%S\")\n",
        "    except ValueError:\n",
        "        print(\"Warning: Invalid start time format. Using 00:00:00\")\n",
        "        start_dt = datetime.strptime(\"00:00:00\", \"%H:%M:%S\")\n",
        "\n",
        "    # Calculate step for frame skipping\n",
        "    step = 1\n",
        "    processing_fps = video_fps\n",
        "    if target_fps is not None and target_fps > 0:\n",
        "        step = max(1, int(round(video_fps / target_fps)))\n",
        "        processing_fps = video_fps / step\n",
        "        print(f\"Processing at ~{processing_fps:.2f} FPS (Video FPS: {video_fps:.2f}, Step: {step})\")\n",
        "\n",
        "    frames_per_chunk_video = int(video_fps * chunk_seconds)\n",
        "    chunk_index = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        frames_to_process = []\n",
        "        frames_read_count = 0\n",
        "\n",
        "        # Collect frames covering the duration of 'chunk_seconds'\n",
        "        while frames_read_count < frames_per_chunk_video:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret: break\n",
        "\n",
        "            # Maintain continuity of step count across chunks relative to global grid\n",
        "            global_frame_idx = (chunk_index * frames_per_chunk_video) + frames_read_count\n",
        "\n",
        "            if global_frame_idx % step == 0:\n",
        "                frames_to_process.append(frame)\n",
        "\n",
        "            frames_read_count += 1\n",
        "\n",
        "        if frames_read_count == 0: break\n",
        "\n",
        "        # Calculate current chunk start time (Time of Day)\n",
        "        chunk_offset_seconds = chunk_index * chunk_seconds\n",
        "        current_chunk_dt = start_dt + timedelta(seconds=chunk_offset_seconds)\n",
        "\n",
        "        print(f\"--> Processing Chunk {chunk_index} (Time: {current_chunk_dt.strftime('%H:%M:%S')})...\")\n",
        "\n",
        "        results, fail_rate = pipeline.process_chunk(frames_to_process, current_chunk_dt, processing_fps)\n",
        "\n",
        "        # Include start time in filename\n",
        "        time_label = current_chunk_dt.strftime(\"%H-%M-%S\")\n",
        "        chunk_filename = os.path.join(output_dir, f\"{time_label}.json\")\n",
        "\n",
        "        output_payload = {\n",
        "            \"chunk_id\": chunk_index,\n",
        "            \"start_time\": current_chunk_dt.strftime(\"%H:%M:%S\"),\n",
        "            \"quality_fail_rate\": round(fail_rate, 3),\n",
        "            \"processing_fps\": round(processing_fps, 2),\n",
        "            \"frames\": results\n",
        "        }\n",
        "\n",
        "        with open(chunk_filename, 'w') as f:\n",
        "            json.dump(output_payload, f, indent=4)\n",
        "\n",
        "        chunk_index += 1\n",
        "\n",
        "    cap.release()\n",
        "    print(\"Workflow finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filter_obj = FrameQualityFilter(failure_ratio_limit=0.5)\n",
        "pipe = DogBehaviorPipeline(\"/content/meld-ml/yolo_v26n_Dog.pt\", \"gpt-5.1\", filter_obj, question = \"What the dog is doing?\", heartbeat_interval=15, stability_threshold=2)"
      ],
      "metadata": {
        "id": "u--ef7hdqONF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_video_to_json(\n",
        "    \"/content/drive/MyDrive/RingXMELD/Product/POC/Videos/DONE/Annika Bremhorst/vid8.mp4\",\n",
        "    pipe,\n",
        "    output_dir=\"/content/drive/MyDrive/RingXMELD/AI/Experiments/video_run\",\n",
        "    chunk_seconds=30,\n",
        "    target_fps=1,\n",
        "    video_start_time=\"00:00:00\")"
      ],
      "metadata": {
        "id": "N2gdnJvi1WSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a5dfbee",
        "outputId": "6b727631-e4a0-4a92-bb3d-a9074a9634e8"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/RingXMELD/AI/Experiments/video_run\"\n",
        "json_files = sorted(glob.glob(os.path.join(output_dir, \"*.json\")))\n",
        "\n",
        "total_frames = 0\n",
        "dog_frames = 0\n",
        "kept_frames = 0\n",
        "confidences = []\n",
        "timeline = []\n",
        "behavior_counts = {}\n",
        "\n",
        "print(f\"Found {len(json_files)} chunk files in {output_dir}.\")\n",
        "\n",
        "for jf in json_files:\n",
        "    try:\n",
        "        with open(jf, 'r') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        for frame in data.get('frames', []):\n",
        "            total_frames += 1\n",
        "\n",
        "            if frame.get('status') == 'Kept':\n",
        "                kept_frames += 1\n",
        "\n",
        "            # Check for YOLO detection\n",
        "            if frame.get('yolo_output'):\n",
        "                dog_frames += 1\n",
        "                confidences.append(frame['yolo_output']['conf'])\n",
        "\n",
        "                # Aggregate Behavior Counts\n",
        "                label = frame['yolo_output']['label']\n",
        "                behavior_counts[label] = behavior_counts.get(label, 0) + 1\n",
        "\n",
        "            # Check for LLM description\n",
        "            if frame.get('llm_description'):\n",
        "                desc = frame['llm_description']\n",
        "                # Simple check to filter out obvious API errors from the timeline display if desired\n",
        "                # if \"Error\" not in desc:\n",
        "                timeline.append({\n",
        "                     \"time\": frame['timestamp'],\n",
        "                     \"description\": desc\n",
        "                 })\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {jf}: {e}\")\n",
        "\n",
        "# --- Display Statistics ---\n",
        "avg_conf = np.mean(confidences) if confidences else 0\n",
        "none_count = total_frames - dog_frames\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"       DOG BEHAVIOR ANALYTICS       \")\n",
        "print(\"=\"*40)\n",
        "print(f\"Total Frames Processed: {total_frames}\")\n",
        "print(f\"Frames with Dog Detected: {dog_frames} ({ (dog_frames/total_frames*100) if total_frames else 0:.1f}%)\")\n",
        "print(f\"Frames with 'Kept' Status: {kept_frames}\")\n",
        "print(f\"Average Confidence Score: {avg_conf:.3f}\")\n",
        "\n",
        "print(\"-\" * 20)\n",
        "print(\"YOLO Behavior Distribution (Including 'None'):\")\n",
        "sorted_behaviors = sorted(behavior_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "if total_frames > 0:\n",
        "    for label, count in sorted_behaviors:\n",
        "        pct = (count / total_frames) * 100\n",
        "        print(f\"  - {label}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "    pct_none = (none_count / total_frames) * 100\n",
        "    print(f\"  - None (No Detection): {none_count} ({pct_none:.1f}%)\")\n",
        "else:\n",
        "    print(\"  No frames processed.\")\n",
        "\n",
        "print(\"-\" * 20)\n",
        "print(\"YOLO Behavior Distribution (Excluding 'None'):\")\n",
        "if dog_frames > 0:\n",
        "    for label, count in sorted_behaviors:\n",
        "        pct = (count / dog_frames) * 100\n",
        "        print(f\"  - {label}: {count} ({pct:.1f}%)\")\n",
        "else:\n",
        "    print(\"  No dog behaviors detected.\")\n",
        "\n",
        "print(\"=\"*40)\n",
        "\n",
        "# --- Display Timeline ---\n",
        "print(\"\\nBehavior Timeline:\")\n",
        "print(\"-\"*40)\n",
        "if not timeline:\n",
        "    print(\"No behavior descriptions recorded.\")\n",
        "else:\n",
        "    for event in timeline:\n",
        "        # Adjusted format to handle string timestamps (HH:MM:SS.mm)\n",
        "        print(f\"[{event['time']}] {event['description']}\")\n",
        "print(\"-\"*40)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running analysis verification (IoU override)...\n",
            "Found 5 chunk files in /content/drive/MyDrive/RingXMELD/AI/Experiments/video_run.\n",
            "\n",
            "========================================\n",
            "       DOG BEHAVIOR ANALYTICS       \n",
            "========================================\n",
            "Total Frames Processed: 123\n",
            "Frames with Dog Detected: 21 (17.1%)\n",
            "Frames with 'Kept' Status: 2\n",
            "Average Confidence Score: 0.644\n",
            "--------------------\n",
            "YOLO Behavior Distribution (Including 'None'):\n",
            "  - unknown: 9 (7.3%)\n",
            "  - sniff: 7 (5.7%)\n",
            "  - walk: 3 (2.4%)\n",
            "  - stand: 2 (1.6%)\n",
            "  - None (No Detection): 102 (82.9%)\n",
            "--------------------\n",
            "YOLO Behavior Distribution (Excluding 'None'):\n",
            "  - unknown: 9 (42.9%)\n",
            "  - sniff: 7 (33.3%)\n",
            "  - walk: 3 (14.3%)\n",
            "  - stand: 2 (9.5%)\n",
            "========================================\n",
            "\n",
            "Behavior Timeline:\n",
            "----------------------------------------\n",
            "[00:00:51.01] The dog is standing on the rug, head lowered toward the floor, appearing to sniff or investigate the area in front of its front paws.\n",
            "[00:00:57.02] The dog is standing or sitting still near the bottom center of the image, facing toward the room.\n",
            "[00:01:08.00] The dog is standing near the left edge of the frame, partly out of view, and appears to be looking into the room.\n",
            "[00:01:17.01] In this image, the dog is lying down or resting near the bottom‑right corner of the frame.\n",
            "[00:01:18.01] The dog is lying down on the floor near the bottom edge of the image.\n",
            "[00:01:19.01] The dog is standing (or sitting very close) near the camera, facing into the room. No other clear activity is visible.\n",
            "[00:01:27.02] The dog is standing (or sitting very upright) near the camera, facing into the room and looking ahead. No other clear action is visible.\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ]
}